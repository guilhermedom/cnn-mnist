{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "ZuvPZBW20xuy",
        "outputId": "ba2f640f-f1c9-4322-e00e-2169617913f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/raw/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From c:\\Users\\guilh\\.conda\\envs\\bf_minhashing\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "0 0.1046\n",
            "100 0.8446\n",
            "200 0.907\n",
            "300 0.9301\n",
            "400 0.9387\n",
            "500 0.9419\n",
            "600 0.9552\n",
            "700 0.9553\n",
            "800 0.9599\n",
            "900 0.9647\n",
            "1000 0.9625\n",
            "1100 0.9686\n",
            "1200 0.9705\n",
            "1300 0.9709\n",
            "1400 0.9731\n",
            "1500 0.974\n",
            "1600 0.9745\n",
            "1700 0.9757\n",
            "1800 0.9768\n",
            "1900 0.9767\n",
            "2000 0.977\n",
            "2100 0.9773\n",
            "2200 0.9788\n",
            "2300 0.9786\n",
            "2400 0.9791\n",
            "2500 0.9799\n",
            "2600 0.9809\n",
            "2700 0.9764\n",
            "2800 0.9812\n",
            "2900 0.9817\n",
            "3000 0.9801\n",
            "3100 0.9832\n",
            "3200 0.9822\n",
            "3300 0.9823\n",
            "3400 0.9842\n",
            "3500 0.983\n",
            "3600 0.983\n",
            "3700 0.9835\n",
            "3800 0.9835\n",
            "3900 0.9842\n",
            "4000 0.9834\n",
            "4100 0.9823\n",
            "4200 0.986\n",
            "4300 0.9852\n",
            "4400 0.986\n",
            "4500 0.9848\n",
            "4600 0.9851\n",
            "4700 0.9855\n",
            "4800 0.9859\n",
            "4900 0.9863\n",
            "5000 0.9858\n",
            "5100 0.9861\n",
            "5200 0.9875\n",
            "5300 0.9855\n",
            "5400 0.9875\n",
            "5500 0.9882\n",
            "5600 0.988\n",
            "5700 0.9874\n",
            "5800 0.9877\n",
            "5900 0.9872\n",
            "6000 0.9877\n",
            "6100 0.9862\n",
            "6200 0.9869\n",
            "6300 0.9869\n",
            "6400 0.9876\n",
            "6500 0.9884\n",
            "6600 0.9878\n",
            "6700 0.9883\n",
            "6800 0.9885\n",
            "6900 0.9886\n",
            "7000 0.9876\n",
            "7100 0.9877\n",
            "7200 0.9885\n",
            "7300 0.9881\n",
            "7400 0.9886\n",
            "7500 0.9877\n",
            "7600 0.9905\n",
            "7700 0.9901\n",
            "7800 0.9906\n",
            "7900 0.9889\n",
            "8000 0.9887\n",
            "8100 0.9899\n",
            "8200 0.989\n",
            "8300 0.9907\n",
            "8400 0.9888\n",
            "8500 0.9898\n",
            "8600 0.9903\n",
            "8700 0.9897\n",
            "8800 0.9897\n",
            "8900 0.9894\n",
            "9000 0.9898\n",
            "9100 0.9894\n",
            "9200 0.9906\n",
            "9300 0.988\n",
            "9400 0.9891\n",
            "9500 0.9904\n",
            "9600 0.9907\n",
            "9700 0.9898\n",
            "9800 0.9909\n",
            "9900 0.9894\n",
            "10000 0.9901\n",
            "10100 0.9905\n",
            "10200 0.99\n",
            "10300 0.9902\n",
            "10400 0.9905\n",
            "10500 0.9899\n",
            "10600 0.9894\n",
            "10700 0.9898\n",
            "10800 0.991\n",
            "10900 0.9904\n",
            "11000 0.9898\n",
            "11100 0.9909\n",
            "11200 0.9907\n",
            "11300 0.9904\n",
            "11400 0.9911\n",
            "11500 0.9907\n",
            "11600 0.9906\n",
            "11700 0.9911\n",
            "11800 0.9913\n",
            "11900 0.99\n",
            "12000 0.9909\n",
            "12100 0.9911\n",
            "12200 0.9917\n",
            "12300 0.9909\n",
            "12400 0.9913\n",
            "12500 0.9913\n",
            "12600 0.9917\n",
            "12700 0.991\n",
            "12800 0.9904\n",
            "12900 0.9908\n",
            "13000 0.9914\n",
            "13100 0.9909\n",
            "13200 0.9911\n",
            "13300 0.9903\n",
            "13400 0.9918\n",
            "13500 0.9918\n",
            "13600 0.9918\n",
            "13700 0.9914\n",
            "13800 0.9914\n",
            "13900 0.9906\n",
            "14000 0.9903\n",
            "14100 0.9909\n",
            "14200 0.9914\n",
            "14300 0.9917\n",
            "14400 0.992\n",
            "14500 0.9918\n",
            "14600 0.9919\n",
            "14700 0.9925\n",
            "14800 0.9909\n",
            "14900 0.993\n",
            "15000 0.9925\n"
          ]
        }
      ],
      "source": [
        "# CNN with complete topology (convolutional layer with 32 feature maps and 5x5 filter ->\n",
        "# convolutional layer with 64 feature maps and 5x5 filter -> fully connected layer -> \n",
        "# dropout -> fully connected layer -> softmax)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import input_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"../data/raw/MNIST_data/\", one_hot=True)\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Input layer\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.float32, [None, 10],  name='y_')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "keep_prob  = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Training steps\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    max_steps = 15000\n",
        "    for step in range(max_steps):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "        if (step % 100) == 0:\n",
        "            print(step, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "    print(max_steps, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "Vvrznsaf3bpj",
        "outputId": "4eece059-5896-4a9a-8a8a-f7a7221c7cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0 0.084\n",
            "100 0.8233\n",
            "200 0.8983\n",
            "300 0.9227\n",
            "400 0.9357\n",
            "500 0.9429\n",
            "600 0.9437\n",
            "700 0.9516\n",
            "800 0.9529\n",
            "900 0.9607\n",
            "1000 0.9619\n",
            "1100 0.9621\n",
            "1200 0.9648\n",
            "1300 0.9664\n",
            "1400 0.9684\n",
            "1500 0.9711\n",
            "1600 0.9726\n",
            "1700 0.9736\n",
            "1800 0.9736\n",
            "1900 0.9753\n",
            "2000 0.9757\n",
            "2100 0.9765\n",
            "2200 0.9753\n",
            "2300 0.978\n",
            "2400 0.9783\n",
            "2500 0.9785\n",
            "2600 0.9785\n",
            "2700 0.9802\n",
            "2800 0.9795\n",
            "2900 0.9812\n",
            "3000 0.9817\n",
            "3100 0.9814\n",
            "3200 0.9793\n",
            "3300 0.9809\n",
            "3400 0.9818\n",
            "3500 0.9822\n",
            "3600 0.9832\n",
            "3700 0.9814\n",
            "3800 0.9823\n",
            "3900 0.9842\n",
            "4000 0.9844\n",
            "4100 0.9848\n",
            "4200 0.9853\n",
            "4300 0.985\n",
            "4400 0.9845\n",
            "4500 0.982\n",
            "4600 0.9851\n",
            "4700 0.9846\n",
            "4800 0.9866\n",
            "4900 0.9839\n",
            "5000 0.9849\n",
            "5100 0.9847\n",
            "5200 0.9863\n",
            "5300 0.986\n",
            "5400 0.9857\n",
            "5500 0.9851\n",
            "5600 0.9862\n",
            "5700 0.9873\n",
            "5800 0.9864\n",
            "5900 0.9877\n",
            "6000 0.986\n",
            "6100 0.986\n",
            "6200 0.9878\n",
            "6300 0.9877\n",
            "6400 0.9873\n",
            "6500 0.9878\n",
            "6600 0.987\n",
            "6700 0.9873\n",
            "6800 0.9878\n",
            "6900 0.9881\n",
            "7000 0.9881\n",
            "7100 0.9868\n",
            "7200 0.9888\n",
            "7300 0.9874\n",
            "7400 0.9881\n",
            "7500 0.9901\n",
            "7600 0.9875\n",
            "7700 0.9891\n",
            "7800 0.9875\n",
            "7900 0.99\n",
            "8000 0.9897\n",
            "8100 0.989\n",
            "8200 0.9897\n",
            "8300 0.9897\n",
            "8400 0.9884\n",
            "8500 0.9893\n",
            "8600 0.9887\n",
            "8700 0.9901\n",
            "8800 0.9887\n",
            "8900 0.9897\n",
            "9000 0.9899\n",
            "9100 0.9886\n",
            "9200 0.9896\n",
            "9300 0.9893\n",
            "9400 0.9897\n",
            "9500 0.9903\n",
            "9600 0.9901\n",
            "9700 0.9886\n",
            "9800 0.9903\n",
            "9900 0.9902\n",
            "10000 0.9897\n",
            "10100 0.9896\n",
            "10200 0.9901\n",
            "10300 0.9906\n",
            "10400 0.9901\n",
            "10500 0.9896\n",
            "10600 0.9892\n",
            "10700 0.9899\n",
            "10800 0.9905\n",
            "10900 0.9901\n",
            "11000 0.9904\n",
            "11100 0.9905\n",
            "11200 0.9901\n",
            "11300 0.9886\n",
            "11400 0.9884\n",
            "11500 0.9898\n",
            "11600 0.9898\n",
            "11700 0.9911\n",
            "11800 0.9898\n",
            "11900 0.99\n",
            "12000 0.9898\n",
            "12100 0.9912\n",
            "12200 0.9908\n",
            "12300 0.992\n",
            "12400 0.9916\n",
            "12500 0.9913\n",
            "12600 0.991\n",
            "12700 0.9905\n",
            "12800 0.991\n",
            "12900 0.9902\n",
            "13000 0.9907\n",
            "13100 0.9914\n",
            "13200 0.9901\n",
            "13300 0.9908\n",
            "13400 0.9912\n",
            "13500 0.9913\n",
            "13600 0.9907\n",
            "13700 0.9908\n",
            "13800 0.9905\n",
            "13900 0.9917\n",
            "14000 0.992\n",
            "14100 0.9909\n",
            "14200 0.9914\n",
            "14300 0.9906\n",
            "14400 0.9921\n",
            "14500 0.992\n",
            "14600 0.9905\n",
            "14700 0.9915\n",
            "14800 0.9912\n",
            "14900 0.9913\n",
            "15000 0.9921\n"
          ]
        }
      ],
      "source": [
        "# CNN 1st variation (convolutional layer with 25 feature maps and 5x5 filter -> convolutional\n",
        "# layer with 50 feature maps and 5x5 filter -> fully connected layer -> dropout ->\n",
        "# fully connected layer -> softmax)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import input_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"../data/raw/MNIST_data/\", one_hot=True)\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Input layer\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.float32, [None, 10],  name='y_')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([5, 5, 1, 25])\n",
        "b_conv1 = bias_variable([25])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([5, 5, 25, 50])\n",
        "b_conv2 = bias_variable([50])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 50])\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 50, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "keep_prob  = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Training steps\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    max_steps = 15000\n",
        "    for step in range(max_steps):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "        if (step % 100) == 0:\n",
        "            print(step, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "    print(max_steps, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "qTT4Clk69Tq4",
        "outputId": "617e9763-949e-4b3b-99b9-9bb6ff3ec870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0 0.0785\n",
            "100 0.7933\n",
            "200 0.8811\n",
            "300 0.9126\n",
            "400 0.9228\n",
            "500 0.9331\n",
            "600 0.938\n",
            "700 0.944\n",
            "800 0.9443\n",
            "900 0.9499\n",
            "1000 0.9551\n",
            "1100 0.9508\n",
            "1200 0.9589\n",
            "1300 0.9603\n",
            "1400 0.9614\n",
            "1500 0.9649\n",
            "1600 0.9684\n",
            "1700 0.9657\n",
            "1800 0.9681\n",
            "1900 0.9709\n",
            "2000 0.9696\n",
            "2100 0.9721\n",
            "2200 0.9722\n",
            "2300 0.9715\n",
            "2400 0.9764\n",
            "2500 0.9732\n",
            "2600 0.9759\n",
            "2700 0.9768\n",
            "2800 0.9774\n",
            "2900 0.9792\n",
            "3000 0.9795\n",
            "3100 0.9795\n",
            "3200 0.9778\n",
            "3300 0.9803\n",
            "3400 0.9793\n",
            "3500 0.9809\n",
            "3600 0.9796\n",
            "3700 0.9809\n",
            "3800 0.9814\n",
            "3900 0.9819\n",
            "4000 0.9817\n",
            "4100 0.9832\n",
            "4200 0.9838\n",
            "4300 0.9822\n",
            "4400 0.9819\n",
            "4500 0.983\n",
            "4600 0.9841\n",
            "4700 0.9825\n",
            "4800 0.9852\n",
            "4900 0.9836\n",
            "5000 0.985\n",
            "5100 0.9854\n",
            "5200 0.9849\n",
            "5300 0.9856\n",
            "5400 0.9851\n",
            "5500 0.9847\n",
            "5600 0.9859\n",
            "5700 0.9862\n",
            "5800 0.9865\n",
            "5900 0.9868\n",
            "6000 0.986\n",
            "6100 0.9869\n",
            "6200 0.9864\n",
            "6300 0.9865\n",
            "6400 0.9865\n",
            "6500 0.9863\n",
            "6600 0.9869\n",
            "6700 0.9878\n",
            "6800 0.9868\n",
            "6900 0.9878\n",
            "7000 0.9867\n",
            "7100 0.9875\n",
            "7200 0.9882\n",
            "7300 0.9885\n",
            "7400 0.9886\n",
            "7500 0.9891\n",
            "7600 0.9884\n",
            "7700 0.9887\n",
            "7800 0.9887\n",
            "7900 0.9891\n",
            "8000 0.9881\n",
            "8100 0.9885\n",
            "8200 0.9878\n",
            "8300 0.9883\n",
            "8400 0.9876\n",
            "8500 0.9882\n",
            "8600 0.9885\n",
            "8700 0.9888\n",
            "8800 0.9879\n",
            "8900 0.9891\n",
            "9000 0.9891\n",
            "9100 0.9894\n",
            "9200 0.9899\n",
            "9300 0.9892\n",
            "9400 0.9882\n",
            "9500 0.9889\n",
            "9600 0.9895\n",
            "9700 0.9884\n",
            "9800 0.989\n",
            "9900 0.9889\n",
            "10000 0.9895\n",
            "10100 0.9893\n",
            "10200 0.99\n",
            "10300 0.9893\n",
            "10400 0.9887\n",
            "10500 0.9899\n",
            "10600 0.99\n",
            "10700 0.9906\n",
            "10800 0.9902\n",
            "10900 0.989\n",
            "11000 0.9882\n",
            "11100 0.9899\n",
            "11200 0.9899\n",
            "11300 0.9898\n",
            "11400 0.9902\n",
            "11500 0.9902\n",
            "11600 0.9896\n",
            "11700 0.99\n",
            "11800 0.9906\n",
            "11900 0.9903\n",
            "12000 0.9909\n",
            "12100 0.9901\n",
            "12200 0.9913\n",
            "12300 0.9893\n",
            "12400 0.9902\n",
            "12500 0.9911\n",
            "12600 0.9901\n",
            "12700 0.9905\n",
            "12800 0.9899\n",
            "12900 0.9914\n",
            "13000 0.9911\n",
            "13100 0.9908\n",
            "13200 0.9909\n",
            "13300 0.991\n",
            "13400 0.9909\n",
            "13500 0.9912\n",
            "13600 0.9907\n",
            "13700 0.9903\n",
            "13800 0.9907\n",
            "13900 0.9915\n",
            "14000 0.9909\n",
            "14100 0.9917\n",
            "14200 0.9911\n",
            "14300 0.9909\n",
            "14400 0.9905\n",
            "14500 0.9909\n",
            "14600 0.991\n",
            "14700 0.9906\n",
            "14800 0.9911\n",
            "14900 0.992\n",
            "15000 0.9917\n"
          ]
        }
      ],
      "source": [
        "# CNN 2nd variation (convolutional layer with 25 feature maps and 3x3 filter -> convolutional\n",
        "# layer with 50 feature maps and 4x4 filter -> fully connected layer -> dropout -> fully\n",
        "# connected layer -> softmax)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import input_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"../data/raw/MNIST_data/\", one_hot=True)\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Input layer\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.float32, [None, 10],  name='y_')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([3, 3, 1, 25])\n",
        "b_conv1 = bias_variable([25])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([4, 4, 25, 50])\n",
        "b_conv2 = bias_variable([50])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7* 50])\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 50, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "keep_prob  = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Training steps\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    max_steps = 15000\n",
        "    for step in range(max_steps):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "        if (step % 100) == 0:\n",
        "            print(step, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "    print(max_steps, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "q2dsdu52-Ole",
        "outputId": "60f6d548-e173-4a19-9497-e46e6aba51c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0 0.1151\n",
            "100 0.4743\n",
            "200 0.7299\n",
            "300 0.7924\n",
            "400 0.8525\n",
            "500 0.8674\n",
            "600 0.8869\n",
            "700 0.8974\n",
            "800 0.9033\n",
            "900 0.9112\n",
            "1000 0.9114\n",
            "1100 0.9175\n",
            "1200 0.9206\n",
            "1300 0.9254\n",
            "1400 0.927\n",
            "1500 0.9302\n",
            "1600 0.9329\n",
            "1700 0.932\n",
            "1800 0.9382\n",
            "1900 0.9394\n",
            "2000 0.9431\n",
            "2100 0.9425\n",
            "2200 0.9399\n",
            "2300 0.9475\n",
            "2400 0.9481\n",
            "2500 0.9453\n",
            "2600 0.9514\n",
            "2700 0.9525\n",
            "2800 0.9519\n",
            "2900 0.9547\n",
            "3000 0.9568\n",
            "3100 0.9557\n",
            "3200 0.9595\n",
            "3300 0.9579\n",
            "3400 0.9603\n",
            "3500 0.9595\n",
            "3600 0.9589\n",
            "3700 0.9615\n",
            "3800 0.9623\n",
            "3900 0.9634\n",
            "4000 0.9644\n",
            "4100 0.9653\n",
            "4200 0.967\n",
            "4300 0.9658\n",
            "4400 0.968\n",
            "4500 0.9674\n",
            "4600 0.9681\n",
            "4700 0.9702\n",
            "4800 0.9696\n",
            "4900 0.9712\n",
            "5000 0.9719\n",
            "5100 0.9711\n",
            "5200 0.9735\n",
            "5300 0.971\n",
            "5400 0.9706\n",
            "5500 0.9716\n",
            "5600 0.9717\n",
            "5700 0.9742\n",
            "5800 0.9732\n",
            "5900 0.9743\n",
            "6000 0.9751\n",
            "6100 0.9752\n",
            "6200 0.9752\n",
            "6300 0.9767\n",
            "6400 0.9759\n",
            "6500 0.9777\n",
            "6600 0.9762\n",
            "6700 0.978\n",
            "6800 0.9771\n",
            "6900 0.9775\n",
            "7000 0.9782\n",
            "7100 0.9763\n",
            "7200 0.9764\n",
            "7300 0.9787\n",
            "7400 0.9797\n",
            "7500 0.9793\n",
            "7600 0.9786\n",
            "7700 0.9786\n",
            "7800 0.9778\n",
            "7900 0.9794\n",
            "8000 0.9796\n",
            "8100 0.981\n",
            "8200 0.9798\n",
            "8300 0.9791\n",
            "8400 0.9784\n",
            "8500 0.9812\n",
            "8600 0.9807\n",
            "8700 0.9811\n",
            "8800 0.9801\n",
            "8900 0.9807\n",
            "9000 0.9798\n",
            "9100 0.9809\n",
            "9200 0.9807\n",
            "9300 0.9818\n",
            "9400 0.9823\n",
            "9500 0.9827\n",
            "9600 0.9828\n",
            "9700 0.9798\n",
            "9800 0.9811\n",
            "9900 0.9812\n",
            "10000 0.9811\n",
            "10100 0.9824\n",
            "10200 0.9822\n",
            "10300 0.9824\n",
            "10400 0.982\n",
            "10500 0.9832\n",
            "10600 0.9837\n",
            "10700 0.9828\n",
            "10800 0.984\n",
            "10900 0.9821\n",
            "11000 0.9805\n",
            "11100 0.9835\n",
            "11200 0.9837\n",
            "11300 0.984\n",
            "11400 0.9829\n",
            "11500 0.9834\n",
            "11600 0.9829\n",
            "11700 0.9842\n",
            "11800 0.9841\n",
            "11900 0.9841\n",
            "12000 0.9831\n",
            "12100 0.9835\n",
            "12200 0.9838\n",
            "12300 0.9848\n",
            "12400 0.9838\n",
            "12500 0.9847\n",
            "12600 0.9851\n",
            "12700 0.9847\n",
            "12800 0.9857\n",
            "12900 0.985\n",
            "13000 0.9848\n",
            "13100 0.9855\n",
            "13200 0.9837\n",
            "13300 0.9852\n",
            "13400 0.9854\n",
            "13500 0.9848\n",
            "13600 0.9841\n",
            "13700 0.9848\n",
            "13800 0.9857\n",
            "13900 0.9848\n",
            "14000 0.9837\n",
            "14100 0.9853\n",
            "14200 0.986\n",
            "14300 0.9852\n",
            "14400 0.9858\n",
            "14500 0.9859\n",
            "14600 0.986\n",
            "14700 0.9853\n",
            "14800 0.9861\n",
            "14900 0.9863\n",
            "15000 0.9857\n"
          ]
        }
      ],
      "source": [
        "# CNN 3rd variation (convolutional layer with 25 feature maps and 3x3 filter -> convolutional\n",
        "# layer with 50 feature maps and 4x4 filter -> fully connected layer -> softmax)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import input_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"../data/raw/MNIST_data/\", one_hot=True)\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Input layer\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.float32, [None, 10],  name='y_')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([3, 3, 1, 25])\n",
        "b_conv1 = bias_variable([25])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([4, 4, 25, 50])\n",
        "b_conv2 = bias_variable([50])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 50])\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 50, 10])\n",
        "b_fc1 = bias_variable([10])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "# keep_prob  = tf.placeholder(tf.float32)\n",
        "# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "# W_fc2 = weight_variable([1024, 10])\n",
        "# b_fc2 = bias_variable([10])\n",
        "\n",
        "# y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Training steps\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    max_steps = 15000\n",
        "    for step in range(max_steps):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "        if (step % 100) == 0:\n",
        "            print(step, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "    print(max_steps, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "QXRTU_tSdt7z",
        "outputId": "18f713ad-ad3f-4291-de07-96e2927cd60e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/raw/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../data/raw/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0 0.084\n",
            "100 0.4402\n",
            "200 0.6688\n",
            "300 0.7919\n",
            "400 0.8273\n",
            "500 0.8619\n",
            "600 0.8679\n",
            "700 0.8859\n",
            "800 0.8987\n",
            "900 0.9036\n",
            "1000 0.9074\n",
            "1100 0.9095\n",
            "1200 0.9166\n",
            "1300 0.9202\n",
            "1400 0.924\n",
            "1500 0.9276\n",
            "1600 0.9315\n",
            "1700 0.9302\n",
            "1800 0.9344\n",
            "1900 0.9355\n",
            "2000 0.9372\n",
            "2100 0.9401\n",
            "2200 0.9428\n",
            "2300 0.9424\n",
            "2400 0.945\n",
            "2500 0.9476\n",
            "2600 0.9475\n",
            "2700 0.9496\n",
            "2800 0.9508\n",
            "2900 0.9508\n",
            "3000 0.9546\n",
            "3100 0.9554\n",
            "3200 0.9553\n",
            "3300 0.9568\n",
            "3400 0.9587\n",
            "3500 0.9572\n",
            "3600 0.9596\n",
            "3700 0.9604\n",
            "3800 0.9589\n",
            "3900 0.9601\n",
            "4000 0.9629\n",
            "4100 0.9632\n",
            "4200 0.9667\n",
            "4300 0.9662\n",
            "4400 0.9648\n",
            "4500 0.9669\n",
            "4600 0.9675\n",
            "4700 0.9709\n",
            "4800 0.9694\n",
            "4900 0.9703\n",
            "5000 0.9702\n",
            "5100 0.9692\n",
            "5200 0.9726\n",
            "5300 0.9705\n",
            "5400 0.9724\n",
            "5500 0.9736\n",
            "5600 0.9725\n",
            "5700 0.9727\n",
            "5800 0.9738\n",
            "5900 0.9746\n",
            "6000 0.9751\n",
            "6100 0.9752\n",
            "6200 0.9752\n",
            "6300 0.9751\n",
            "6400 0.9767\n",
            "6500 0.9763\n",
            "6600 0.9769\n",
            "6700 0.9756\n",
            "6800 0.977\n",
            "6900 0.9784\n",
            "7000 0.9776\n",
            "7100 0.9776\n",
            "7200 0.978\n",
            "7300 0.9772\n",
            "7400 0.9792\n",
            "7500 0.9796\n",
            "7600 0.9782\n",
            "7700 0.9806\n",
            "7800 0.9782\n",
            "7900 0.9804\n",
            "8000 0.9787\n",
            "8100 0.9803\n",
            "8200 0.9806\n",
            "8300 0.98\n",
            "8400 0.9794\n",
            "8500 0.9799\n",
            "8600 0.9806\n",
            "8700 0.9815\n",
            "8800 0.9804\n",
            "8900 0.9797\n",
            "9000 0.9818\n",
            "9100 0.9815\n",
            "9200 0.9803\n",
            "9300 0.982\n",
            "9400 0.9822\n",
            "9500 0.9803\n",
            "9600 0.9825\n",
            "9700 0.9811\n",
            "9800 0.9823\n",
            "9900 0.9813\n",
            "10000 0.9801\n",
            "10100 0.9802\n",
            "10200 0.9829\n",
            "10300 0.9794\n",
            "10400 0.9826\n",
            "10500 0.9823\n",
            "10600 0.9817\n",
            "10700 0.9842\n",
            "10800 0.9832\n",
            "10900 0.9827\n",
            "11000 0.9822\n",
            "11100 0.9835\n",
            "11200 0.9836\n",
            "11300 0.9817\n",
            "11400 0.9822\n",
            "11500 0.9843\n",
            "11600 0.9816\n",
            "11700 0.9818\n",
            "11800 0.9848\n",
            "11900 0.9836\n",
            "12000 0.9821\n",
            "12100 0.9822\n",
            "12200 0.983\n",
            "12300 0.9837\n",
            "12400 0.9848\n",
            "12500 0.9838\n",
            "12600 0.9837\n",
            "12700 0.9841\n",
            "12800 0.9847\n",
            "12900 0.9816\n",
            "13000 0.9831\n",
            "13100 0.9837\n",
            "13200 0.9842\n",
            "13300 0.9846\n",
            "13400 0.9848\n",
            "13500 0.9843\n",
            "13600 0.9843\n",
            "13700 0.9836\n",
            "13800 0.9826\n",
            "13900 0.984\n",
            "14000 0.9845\n",
            "14100 0.9844\n",
            "14200 0.9842\n",
            "14300 0.9845\n",
            "14400 0.9855\n",
            "14500 0.9846\n",
            "14600 0.9851\n",
            "14700 0.9859\n",
            "14800 0.9859\n",
            "14900 0.9844\n",
            "15000 0.9847\n"
          ]
        }
      ],
      "source": [
        "# CNN 4th variation (convolutional layer with 25 feature maps and 3x3 filter -> convolutional\n",
        "# layer with 50 feature maps and 4x4 filter -> dropout -> fully connected layer -> softmax)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import input_data\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"../data/raw/MNIST_data/\", one_hot=True)\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Input layer\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "y_ = tf.placeholder(tf.float32, [None, 10],  name='y_')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([3, 3, 1, 25])\n",
        "b_conv1 = bias_variable([25])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([4, 4, 25, 50])\n",
        "b_conv2 = bias_variable([50])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 50])\n",
        "\n",
        "# W_fc1 = weight_variable([7 * 7 * 50, 1024])\n",
        "# b_fc1 = bias_variable([1024])\n",
        "\n",
        "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "keep_prob  = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_pool2_flat, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "W_fc2 = weight_variable([7 * 7 * 50, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_pool2_flat, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Training steps\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    max_steps = 15000\n",
        "    for step in range(max_steps):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "        if (step % 100) == 0:\n",
        "            print(step, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "    print(max_steps, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Exercicio4_RedesNeurais.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('bf_minhashing')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b39e83d7d937e7ce85387f84ef071986d0096be4afb1ae7c3fe160a424884580"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
